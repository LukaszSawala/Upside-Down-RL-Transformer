{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Constants ---\n",
    "STATE_DIM = 105       # antv5 observation dim\n",
    "ACT_DIM = 8           # antv5 action dim\n",
    "MAX_LENGTH = 20       # DT context window\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "EPOCHS = 20\n",
    "DATA_PATH = \"../data/processed/concatenated_data.hdf5\"\n",
    "DT_MODEL_PATH = \"../models/best_DT.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "with h5py.File(DATA_PATH, \"r\") as f:\n",
    "    data = f[\"concatenated_data\"]\n",
    "    actions = data[\"actions\"][:]\n",
    "    observations = data[\"observations\"][:]\n",
    "    rewards_to_go = data[\"rewards_to_go\"][:]\n",
    "    time_to_go = data[\"time_to_go\"][:]\n",
    "\n",
    "rewards_to_go = rewards_to_go.reshape(-1, 1)\n",
    "time_to_go = time_to_go.reshape(-1, 1)\n",
    "\n",
    "# Combine into (state, rtg, timestep) for DT\n",
    "X = np.concatenate((observations, rewards_to_go, time_to_go), axis=-1)\n",
    "y = actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split Data into Sequences (Necessary for DT) ---\n",
    "def create_sequences(X, y, seq_len=MAX_LENGTH):\n",
    "    num_sequences = len(X) - seq_len + 1\n",
    "    sequences_X, sequences_y = [], []\n",
    "    for i in range(num_sequences):\n",
    "        sequences_X.append(X[i:i+seq_len])\n",
    "        sequences_y.append(y[i:i+seq_len])\n",
    "    return np.stack(sequences_X), np.stack(sequences_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DataLoader ---\n",
    "def create_dataloaders(X_train, X_test, X_val, y_train, y_test, y_val, batch_size=BATCH_SIZE):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),  # (states, rtg, timesteps)\n",
    "        torch.FloatTensor(y_train)   # actions\n",
    "    )\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Decision Transformer Setup ---\n",
    "config = DecisionTransformerConfig(\n",
    "    state_dim=STATE_DIM,\n",
    "    act_dim=ACT_DIM,\n",
    "    max_length=MAX_LENGTH,\n",
    "    n_positions=MAX_LENGTH,\n",
    ")\n",
    "model = DecisionTransformerModel(config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "def train_dt(model, train_loader, val_loader, epochs=EPOCHS):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for states_actions, targets in train_loader:\n",
    "            # states_actions: (batch, seq_len, STATE_DIM + 2)\n",
    "            # targets: (batch, seq_len, ACT_DIM)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Split inputs into states, rtg, timesteps\n",
    "            states = states_actions[:, :, :STATE_DIM]\n",
    "            rtg = states_actions[:, :, STATE_DIM].unsqueeze(-1)\n",
    "            timesteps = states_actions[:, :, STATE_DIM+1].unsqueeze(-1)\n",
    "            \n",
    "            # Forward pass (DT predicts actions)\n",
    "            outputs = model(\n",
    "                states=states,\n",
    "                actions=targets,  # DT uses past actions for prediction\n",
    "                rewards_to_go=rtg,\n",
    "                timesteps=timesteps.long(),\n",
    "            ).logits\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate_dt(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), DT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dt(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for states_actions, targets in loader:\n",
    "            states = states_actions[:, :, :STATE_DIM]\n",
    "            rtg = states_actions[:, :, STATE_DIM].unsqueeze(-1)\n",
    "            timesteps = states_actions[:, :, STATE_DIM+1].unsqueeze(-1)\n",
    "            \n",
    "            outputs = model(\n",
    "                states=states,\n",
    "                actions=targets,\n",
    "                rewards_to_go=rtg,\n",
    "                timesteps=timesteps.long(),\n",
    "            ).logits\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main ---\n",
    "\n",
    "X, y = load_data()\n",
    "\n",
    "# Split into sequences\n",
    "X_seq, y_seq = create_sequences(X, y)\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X_seq, y_seq)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val\n",
    ")\n",
    "\n",
    "# Train DT\n",
    "train_dt(model, train_loader, val_loader)\n",
    "\n",
    "# Test\n",
    "test_loss = evaluate_dt(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
