{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4e77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- VARIABLES TO STORE\n",
    "all_actions = []\n",
    "all_observations = []\n",
    "all_rewards_to_go = []\n",
    "all_time_to_go = []\n",
    "all_goal_vector = []\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4313e",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37c3a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [5. 4. 3. 2. 1.]\n",
      "Keys in episode_999: ['actions', 'goal_vector', 'infos', 'observations', 'rewards', 'rewards_to_go', 'terminations', 'time_to_go', 'truncations']\n",
      "episode_999 Action dimensions: (1000, 8)\n",
      "Observation dimensions: (1000, 27)\n",
      "Reward dimensions: (1000,)\n",
      "Termination dimensions: (1000,)\n",
      "Truncation dimensions: (1000,)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/external/diverse_medium_maze_data.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, \"r+\") as f:\n",
    "    for key in f.keys():\n",
    "        # del episode_87[\"key-id\"] removing a thing\n",
    "        curr_episode = f[key]\n",
    "\n",
    "        # split observations into the useful and useless observations (1 more observation than action) if not done before\n",
    "        actions = curr_episode[\"actions\"][:]\n",
    "        infos = curr_episode[\"infos\"]\n",
    "        observations = curr_episode[\"observations\"][\"observation\"][:-1]\n",
    "        goal_locations = curr_episode[\"observations\"][\"desired_goal\"][:-1]\n",
    "        current_locations = curr_episode[\"observations\"][\"achieved_goal\"][:-1]\n",
    "        rewards = curr_episode[\"rewards\"][:]\n",
    "        terminations = curr_episode[\"terminations\"][:]\n",
    "        truncations = curr_episode[\"truncations\"][:]\n",
    "\n",
    "        reward_sum = np.sum(rewards)\n",
    "        rewards_to_go = np.zeros_like(rewards)  # desired reward\n",
    "        time_to_go = np.zeros_like(rewards)  # desired horizon\n",
    "        goal_vector = np.zeros_like(goal_locations)  # goal vector\n",
    "        for i in range(len(rewards)):\n",
    "            # save the rewards-to-go\n",
    "            rewards_to_go[i] = reward_sum\n",
    "            reward_sum -= rewards[i]\n",
    "            # save the time-to-go\n",
    "            time_to_go[i] = len(rewards) - i\n",
    "            # save the goal vector (extra t)\n",
    "            goal_x, goal_y = goal_locations[i]\n",
    "            current_x, current_y = current_locations[i]\n",
    "            goal_vector[i] = np.array([goal_x - current_x, goal_y - current_y])\n",
    "\n",
    "        if \"rewards_to_go\" not in curr_episode.keys():\n",
    "            curr_episode[\"rewards_to_go\"] = rewards_to_go\n",
    "            curr_episode[\"time_to_go\"] = time_to_go\n",
    "            curr_episode[\"goal_vector\"] = goal_vector\n",
    "\n",
    "        # save the stuff\n",
    "        all_actions.append(actions)\n",
    "        all_observations.append(observations)\n",
    "        all_rewards_to_go.append(rewards_to_go)\n",
    "        all_time_to_go.append(time_to_go)\n",
    "        all_goal_vector.append(goal_vector)\n",
    "\n",
    "    print(rewards[-5:], rewards_to_go[-5:], time_to_go[-5:])\n",
    "    # Dimensions:\n",
    "    print(f\"Keys in {key}:\", list(curr_episode.keys()))\n",
    "    print(f\"{key} Action dimensions: {actions.shape}\")\n",
    "    print(f\"Observation dimensions: {observations.shape}\")\n",
    "    print(f\"Reward dimensions: {rewards.shape}\")\n",
    "    print(f\"Termination dimensions: {terminations.shape}\")\n",
    "    print(f\"Truncation dimensions: {truncations.shape}\")\n",
    "print(len(all_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40366e79",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b95fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] [5. 4. 3. 2. 1.] [5. 4. 3. 2. 1.]\n",
      "Keys in episode_999: ['actions', 'goal_vector', 'infos', 'observations', 'rewards', 'rewards_to_go', 'terminations', 'time_to_go', 'truncations']\n",
      "episode_999 Action dimensions: (1000, 8)\n",
      "Observation dimensions: (1000, 27)\n",
      "Reward dimensions: (1000,)\n",
      "Termination dimensions: (1000,)\n",
      "Truncation dimensions: (1000,)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/external/play_medium_maze_data.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, \"r+\") as f:\n",
    "    for key in f.keys():\n",
    "        # del episode_87[\"key-id\"] removing a thing\n",
    "        curr_episode = f[key]\n",
    "\n",
    "        # split observations into the useful and useless observations (1 more observation than action) if not done before\n",
    "        actions = curr_episode[\"actions\"][:]\n",
    "        infos = curr_episode[\"infos\"]\n",
    "        observations = curr_episode[\"observations\"][\"observation\"][:-1]\n",
    "        goal_locations = curr_episode[\"observations\"][\"desired_goal\"][:-1]\n",
    "        current_locations = curr_episode[\"observations\"][\"achieved_goal\"][:-1]\n",
    "        rewards = curr_episode[\"rewards\"][:]\n",
    "        terminations = curr_episode[\"terminations\"][:]\n",
    "        truncations = curr_episode[\"truncations\"][:]\n",
    "\n",
    "        reward_sum = np.sum(rewards)\n",
    "        rewards_to_go = np.zeros_like(rewards)  # desired reward\n",
    "        time_to_go = np.zeros_like(rewards)  # desired horizon\n",
    "        goal_vector = np.zeros_like(goal_locations)  # goal vector\n",
    "        for i in range(len(rewards)):\n",
    "            # save the rewards-to-go\n",
    "            rewards_to_go[i] = reward_sum\n",
    "            reward_sum -= rewards[i]\n",
    "            # save the time-to-go\n",
    "            time_to_go[i] = len(rewards) - i\n",
    "            # save the goal vector (extra t)\n",
    "            goal_x, goal_y = goal_locations[i]\n",
    "            current_x, current_y = current_locations[i]\n",
    "            goal_vector[i] = np.array([goal_x - current_x, goal_y - current_y])\n",
    "\n",
    "        if \"rewards_to_go\" not in curr_episode.keys():\n",
    "            curr_episode[\"rewards_to_go\"] = rewards_to_go\n",
    "            curr_episode[\"time_to_go\"] = time_to_go\n",
    "            curr_episode[\"goal_vector\"] = goal_vector\n",
    "\n",
    "        # save the stuff\n",
    "        all_actions.append(actions)\n",
    "        all_observations.append(observations)\n",
    "        all_rewards_to_go.append(rewards_to_go)\n",
    "        all_time_to_go.append(time_to_go)\n",
    "        all_goal_vector.append(goal_vector)\n",
    "\n",
    "    print(rewards[-5:], rewards_to_go[-5:], time_to_go[-5:])\n",
    "    # Dimensions:\n",
    "    print(f\"Keys in {key}:\", list(curr_episode.keys()))\n",
    "    print(f\"{key} Action dimensions: {actions.shape}\")\n",
    "    print(f\"Observation dimensions: {observations.shape}\")\n",
    "    print(f\"Reward dimensions: {rewards.shape}\")\n",
    "    print(f\"Termination dimensions: {terminations.shape}\")\n",
    "    print(f\"Truncation dimensions: {truncations.shape}\")\n",
    "print(len(all_actions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5053f",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f733a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] [5. 4. 3. 2. 1.] [5. 4. 3. 2. 1.]\n",
      "Keys in episode_999: ['actions', 'goal_vector', 'infos', 'observations', 'rewards', 'rewards_to_go', 'terminations', 'time_to_go', 'truncations']\n",
      "episode_999 Action dimensions: (700, 8)\n",
      "Observation dimensions: (700, 27)\n",
      "Reward dimensions: (700,)\n",
      "Termination dimensions: (700,)\n",
      "Truncation dimensions: (700,)\n",
      "3430\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/external/umaze_diverse_data.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, \"r+\") as f:\n",
    "    for key in f.keys():\n",
    "        # del episode_87[\"key-id\"] removing a thing\n",
    "        curr_episode = f[key]\n",
    "\n",
    "        # split observations into the useful and useless observations (1 more observation than action) if not done before\n",
    "        actions = curr_episode[\"actions\"][:]\n",
    "        infos = curr_episode[\"infos\"]\n",
    "        observations = curr_episode[\"observations\"][\"observation\"][:-1]\n",
    "        goal_locations = curr_episode[\"observations\"][\"desired_goal\"][:-1]\n",
    "        current_locations = curr_episode[\"observations\"][\"achieved_goal\"][:-1]\n",
    "        rewards = curr_episode[\"rewards\"][:]\n",
    "        terminations = curr_episode[\"terminations\"][:]\n",
    "        truncations = curr_episode[\"truncations\"][:]\n",
    "\n",
    "        reward_sum = np.sum(rewards)\n",
    "        rewards_to_go = np.zeros_like(rewards)  # desired reward\n",
    "        time_to_go = np.zeros_like(rewards)  # desired horizon\n",
    "        goal_vector = np.zeros_like(goal_locations)  # goal vector\n",
    "        for i in range(len(rewards)):\n",
    "            # save the rewards-to-go\n",
    "            rewards_to_go[i] = reward_sum\n",
    "            reward_sum -= rewards[i]\n",
    "            # save the time-to-go\n",
    "            time_to_go[i] = len(rewards) - i\n",
    "            # save the goal vector (extra t)\n",
    "            goal_x, goal_y = goal_locations[i]\n",
    "            current_x, current_y = current_locations[i]\n",
    "            goal_vector[i] = np.array([goal_x - current_x, goal_y - current_y])\n",
    "\n",
    "        if \"rewards_to_go\" not in curr_episode.keys():\n",
    "            curr_episode[\"rewards_to_go\"] = rewards_to_go\n",
    "            curr_episode[\"time_to_go\"] = time_to_go\n",
    "            curr_episode[\"goal_vector\"] = goal_vector\n",
    "\n",
    "        # save the stuff\n",
    "        all_actions.append(actions)\n",
    "        all_observations.append(observations)\n",
    "        all_rewards_to_go.append(rewards_to_go)\n",
    "        all_time_to_go.append(time_to_go)\n",
    "        all_goal_vector.append(goal_vector)\n",
    "\n",
    "    print(rewards[-5:], rewards_to_go[-5:], time_to_go[-5:])\n",
    "    # Dimensions:\n",
    "    print(f\"Keys in {key}:\", list(curr_episode.keys()))\n",
    "    print(f\"{key} Action dimensions: {actions.shape}\")\n",
    "    print(f\"Observation dimensions: {observations.shape}\")\n",
    "    print(f\"Reward dimensions: {rewards.shape}\")\n",
    "    print(f\"Termination dimensions: {terminations.shape}\")\n",
    "    print(f\"Truncation dimensions: {truncations.shape}\")\n",
    "print(len(all_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1814972",
   "metadata": {},
   "source": [
    "## Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "452c2feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] [5. 4. 3. 2. 1.] [5. 4. 3. 2. 1.]\n",
      "Keys in episode_999: ['actions', 'goal_vector', 'infos', 'observations', 'rewards', 'rewards_to_go', 'terminations', 'time_to_go', 'truncations']\n",
      "episode_999 Action dimensions: (700, 8)\n",
      "Observation dimensions: (700, 27)\n",
      "Reward dimensions: (700,)\n",
      "Termination dimensions: (700,)\n",
      "Truncation dimensions: (700,)\n",
      "4860\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/external/umaze_play_data.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, \"r+\") as f:\n",
    "    for key in f.keys():\n",
    "        # del episode_87[\"key-id\"] removing a thing\n",
    "        curr_episode = f[key]\n",
    "\n",
    "        # split observations into the useful and useless observations (1 more observation than action) if not done before\n",
    "        actions = curr_episode[\"actions\"][:]\n",
    "        infos = curr_episode[\"infos\"]\n",
    "        observations = curr_episode[\"observations\"][\"observation\"][:-1]\n",
    "        goal_locations = curr_episode[\"observations\"][\"desired_goal\"][:-1]\n",
    "        current_locations = curr_episode[\"observations\"][\"achieved_goal\"][:-1]\n",
    "        rewards = curr_episode[\"rewards\"][:]\n",
    "        terminations = curr_episode[\"terminations\"][:]\n",
    "        truncations = curr_episode[\"truncations\"][:]\n",
    "\n",
    "        reward_sum = np.sum(rewards)\n",
    "        rewards_to_go = np.zeros_like(rewards)  # desired reward\n",
    "        time_to_go = np.zeros_like(rewards)  # desired horizon\n",
    "        goal_vector = np.zeros_like(goal_locations)  # goal vector\n",
    "        for i in range(len(rewards)):\n",
    "            # save the rewards-to-go\n",
    "            rewards_to_go[i] = reward_sum\n",
    "            reward_sum -= rewards[i]\n",
    "            # save the time-to-go\n",
    "            time_to_go[i] = len(rewards) - i\n",
    "            # save the goal vector (extra t)\n",
    "            goal_x, goal_y = goal_locations[i]\n",
    "            current_x, current_y = current_locations[i]\n",
    "            goal_vector[i] = np.array([goal_x - current_x, goal_y - current_y])\n",
    "\n",
    "        if \"rewards_to_go\" not in curr_episode.keys():\n",
    "            curr_episode[\"rewards_to_go\"] = rewards_to_go\n",
    "            curr_episode[\"time_to_go\"] = time_to_go\n",
    "            curr_episode[\"goal_vector\"] = goal_vector\n",
    "\n",
    "        # save the stuff\n",
    "        all_actions.append(actions)\n",
    "        all_observations.append(observations)\n",
    "        all_rewards_to_go.append(rewards_to_go)\n",
    "        all_time_to_go.append(time_to_go)\n",
    "        all_goal_vector.append(goal_vector)\n",
    "\n",
    "    print(rewards[-5:], rewards_to_go[-5:], time_to_go[-5:])\n",
    "    # Dimensions:\n",
    "    print(f\"Keys in {key}:\", list(curr_episode.keys()))\n",
    "    print(f\"{key} Action dimensions: {actions.shape}\")\n",
    "    print(f\"Observation dimensions: {observations.shape}\")\n",
    "    print(f\"Reward dimensions: {rewards.shape}\")\n",
    "    print(f\"Termination dimensions: {terminations.shape}\")\n",
    "    print(f\"Truncation dimensions: {truncations.shape}\")\n",
    "print(len(all_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d991d0",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00668e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data into single vectors\n",
    "all_actions_list = np.concatenate(all_actions, axis=0)\n",
    "all_observations_list = np.concatenate(all_observations, axis=0)\n",
    "all_rewards_to_go_list = np.concatenate(all_rewards_to_go, axis=0)\n",
    "all_time_to_go_list = np.concatenate(all_time_to_go, axis=0)\n",
    "all_goal_vector_list = np.concatenate(all_goal_vector, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70857f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000000, 8) (4000000, 27) (4000000,) (4000000,) (4000000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(all_actions_list.shape, \n",
    "      all_observations_list.shape,\n",
    "      all_rewards_to_go_list.shape,\n",
    "      all_time_to_go_list.shape,\n",
    "      all_goal_vector_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30c64edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"../data/processed/antmaze_merged_concatenated.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c63ac57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated data saved under 'concatenated_data' in ../data/processed/antmaze_merged_concatenated.hdf5\n",
      "Actions shape: (4000000, 8)\n",
      "Observations shape: (4000000, 27)\n",
      "Rewards_to_go shape: (4000000,)\n",
      "Time_to_go shape: (4000000,)\n",
      "Goal_vector shape: (4000000, 2)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(output_file_path, \"w\") as f:\n",
    "    group = f.create_group(\"concatenated_data\")\n",
    "    group.create_dataset(\"actions\", data=all_actions_list)\n",
    "    group.create_dataset(\"observations\", data=all_observations_list)\n",
    "    group.create_dataset(\"rewards_to_go\", data=all_rewards_to_go_list)\n",
    "    group.create_dataset(\"time_to_go\", data=all_time_to_go_list)\n",
    "    group.create_dataset(\"goal_vector\", data=all_goal_vector_list)\n",
    "    print(\n",
    "        f\"Concatenated data saved under 'concatenated_data' in {output_file_path}\"\n",
    "    )\n",
    "    print(f\"Actions shape: {all_actions_list.shape}\")\n",
    "    print(f\"Observations shape: {all_observations_list.shape}\")\n",
    "    print(f\"Rewards_to_go shape: {all_rewards_to_go_list.shape}\")\n",
    "    print(f\"Time_to_go shape: {all_time_to_go_list.shape}\")\n",
    "    print(f\"Goal_vector shape: {all_goal_vector_list.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
