Evaluating base model: UDRLt_MLP0
==================================================
Evaluating d_r: 0
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.008833877955688976 maximum return: 114.68495788529543 average return: 56.930903403346534
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 8.624441128730807e-06 maximum return: 76.98067739240905 average return: 8.181788685923213
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 0.000122736252865774 maximum return: 176.30185645609282 average return: 61.133869684121535
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 5.459764615563972e-06 maximum return: 195.6179804903629 average return: 44.39407092128533
==================================================
Evaluating d_r: 200
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.6558813257610424e-06 maximum return: 217.94548494751288 average return: 96.13892975242825
==================================================
Evaluating d_r: 250
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 9.029066116326936e-08 maximum return: 291.3534463618397 average return: 107.42376994584217
==================================================
Evaluating d_r: 300
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.007033877849778532 maximum return: 309.6166964696039 average return: 111.4045879910569
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 7.0984375779912606e-06 maximum return: 361.34465113120655 average return: 190.10824113021457
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.1180560127326474e-06 maximum return: 24.143849555785284 average return: 2.579278058877111
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.25423092024846666 maximum return: 448.50205713146966 average return: 177.48219960123907
==================================================
Evaluating d_r: 500
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 0.01075913881089587 maximum return: 493.6806324078439 average return: 203.98116358839405
==================================================
Evaluating d_r: 550
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 3.5537410810112106e-05 maximum return: 536.7837413105758 average return: 142.78993132039687
==================================================
Evaluating d_r: 600
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 5.202018642213593e-08 maximum return: 594.4672140721132 average return: 189.21352896492712
==================================================
Evaluating d_r: 650
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.2772409201120873e-05 maximum return: 620.6507433706649 average return: 161.64281417953376
==================================================
Evaluating d_r: 700
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.492624242216961e-08 maximum return: 717.3713778931344 average return: 278.8411725218439
==================================================
Evaluating d_r: 750
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 0.0016746261862703262 maximum return: 755.727721680614 average return: 278.5883766042702
==================================================
Evaluating d_r: 800
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 6.968054612852976e-07 maximum return: 778.2344982708927 average return: 209.80358497691583
==================================================
Evaluating d_r: 850
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 4.5032433558332836e-05 maximum return: 816.4277965220521 average return: 211.2555088024648
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 2.192672645059645e-06 maximum return: 741.6690418428449 average return: 131.26035633858416
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.7983577861219395e-06 maximum return: 712.4120198183326 average return: 95.31863949440532
==================================================
Evaluating d_r: 1000
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.0011876644172720149 maximum return: 843.4846060081026 average return: 201.19701235749017
============================================================
Iteration 1/2

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 332 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (332000, 27)
Actions: (332000, 8)
Rewards-to-Go: (332000,)
Time-to-Go: (332000,)
Goal Vectors: (332000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 193.7606
AVERAGE OBTAINED REWARD PER EPISODE: 295.019803354973
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0189, Val Loss = 0.0120
Best model found! Validation Loss: 0.0120
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0136, Val Loss = 0.0105
Best model found! Validation Loss: 0.0105
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0119, Val Loss = 0.0100
Best model found! Validation Loss: 0.0100
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0110, Val Loss = 0.0096
Best model found! Validation Loss: 0.0096
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0103, Val Loss = 0.0094
Best model found! Validation Loss: 0.0094
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0097, Val Loss = 0.0092
Best model found! Validation Loss: 0.0092
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0092, Val Loss = 0.0091
Best model found! Validation Loss: 0.0091
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0088, Val Loss = 0.0092
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0085, Val Loss = 0.0087
Best model found! Validation Loss: 0.0087
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0082, Val Loss = 0.0091
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0080, Val Loss = 0.0086
Best model found! Validation Loss: 0.0086
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0077, Val Loss = 0.0085
Best model found! Validation Loss: 0.0085
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0075, Val Loss = 0.0086
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0073, Val Loss = 0.0088
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0071, Val Loss = 0.0083
Best model found! Validation Loss: 0.0083
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0070, Val Loss = 0.0085
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0068, Val Loss = 0.0084
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0067, Val Loss = 0.0084
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0065, Val Loss = 0.0082
Best model found! Validation Loss: 0.0082
Epoch 20/100 [Train]: Starting...
Epoch 20/100 [Val  ]: Starting...
Epoch 20/100: Train Loss = 0.0064, Val Loss = 0.0084
Epoch 21/100 [Train]: Starting...
Epoch 21/100 [Val  ]: Starting...
Epoch 21/100: Train Loss = 0.0063, Val Loss = 0.0082
Best model found! Validation Loss: 0.0082
Epoch 22/100 [Train]: Starting...
Epoch 22/100 [Val  ]: Starting...
Epoch 22/100: Train Loss = 0.0062, Val Loss = 0.0084
Epoch 23/100 [Train]: Starting...
Epoch 23/100 [Val  ]: Starting...
Epoch 23/100: Train Loss = 0.0061, Val Loss = 0.0082
Epoch 24/100 [Train]: Starting...
Epoch 24/100 [Val  ]: Starting...
Epoch 24/100: Train Loss = 0.0060, Val Loss = 0.0081
Best model found! Validation Loss: 0.0081
Epoch 25/100 [Train]: Starting...
Epoch 25/100 [Val  ]: Starting...
Epoch 25/100: Train Loss = 0.0059, Val Loss = 0.0085
Epoch 26/100 [Train]: Starting...
Epoch 26/100 [Val  ]: Starting...
Epoch 26/100: Train Loss = 0.0058, Val Loss = 0.0080
Best model found! Validation Loss: 0.0080
Epoch 27/100 [Train]: Starting...
Epoch 27/100 [Val  ]: Starting...
Epoch 27/100: Train Loss = 0.0057, Val Loss = 0.0082
Epoch 28/100 [Train]: Starting...
Epoch 28/100 [Val  ]: Starting...
Epoch 28/100: Train Loss = 0.0056, Val Loss = 0.0083
Epoch 29/100 [Train]: Starting...
Epoch 29/100 [Val  ]: Starting...
Epoch 29/100: Train Loss = 0.0055, Val Loss = 0.0086
Epoch 30/100 [Train]: Starting...
Epoch 30/100 [Val  ]: Starting...
Epoch 30/100: Train Loss = 0.0054, Val Loss = 0.0083
Epoch 31/100 [Train]: Starting...
Epoch 31/100 [Val  ]: Starting...
Epoch 31/100: Train Loss = 0.0053, Val Loss = 0.0083
Epoch 32/100 [Train]: Starting...
Epoch 32/100 [Val  ]: Starting...
Epoch 32/100: Train Loss = 0.0053, Val Loss = 0.0080
Epoch 33/100 [Train]: Starting...
Epoch 33/100 [Val  ]: Starting...
Epoch 33/100: Train Loss = 0.0052, Val Loss = 0.0083
Epoch 34/100 [Train]: Starting...
Epoch 34/100 [Val  ]: Starting...
Epoch 34/100: Train Loss = 0.0051, Val Loss = 0.0082
Epoch 35/100 [Train]: Starting...
Epoch 35/100 [Val  ]: Starting...
Epoch 35/100: Train Loss = 0.0051, Val Loss = 0.0085
Epoch 36/100 [Train]: Starting...
Epoch 36/100 [Val  ]: Starting...
Epoch 36/100: Train Loss = 0.0050, Val Loss = 0.0083
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP1
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 0.0016472167144733522 maximum return: 235.9917529158598 average return: 41.64649593722754
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.7189346878823594e-07 maximum return: 131.8580596132247 average return: 28.711677122663907
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.0003382598561815265 maximum return: 103.42580615116648 average return: 33.55439355252555
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.00026033820288101774 maximum return: 157.03577953892386 average return: 50.309950115084845
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 7.697671180891794e-05 maximum return: 205.30365174825448 average return: 63.962207054423445
==================================================
Evaluating d_r: 250
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.0016718928744794898 maximum return: 288.58593031118016 average return: 99.0271793429088
==================================================
Evaluating d_r: 300
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00010629445676042708 maximum return: 4.1900377188431275 average return: 0.8619121208049249
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.3775866986474964e-05 maximum return: 294.91997332557065 average return: 56.21961313434249
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 5.319946601125888e-05 maximum return: 382.7429139708197 average return: 154.78901768347083
==================================================
Evaluating d_r: 450
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.2375013041125811e-05 maximum return: 462.36411864913225 average return: 120.16945832276322
==================================================
Evaluating d_r: 500
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.0363459301424748e-06 maximum return: 531.0400683443735 average return: 99.2199953280647
==================================================
Evaluating d_r: 550
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.0003108016449792206 maximum return: 578.6839692645565 average return: 211.13200670267707
==================================================
Evaluating d_r: 600
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 0.0007082468027294624 maximum return: 625.6580430492835 average return: 317.0726331681578
==================================================
Evaluating d_r: 650
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00010707302756635027 maximum return: 634.2366689151398 average return: 119.80697509642111
==================================================
Evaluating d_r: 700
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 9.844997174942638e-07 maximum return: 729.5309051013122 average return: 359.37057307670597
==================================================
Evaluating d_r: 750
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 2.070031532954935e-06 maximum return: 720.3643498841432 average return: 190.79860586286574
==================================================
Evaluating d_r: 800
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 7.247544124466937e-05 maximum return: 815.3371632982273 average return: 300.2842951428589
==================================================
Evaluating d_r: 850
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 6.844353942424649e-05 maximum return: 870.3254449949487 average return: 285.40059507526075
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 5.1507461321029215e-05 maximum return: 540.8278669685578 average return: 150.2618560630885
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 2.4547943990578497e-06 maximum return: 690.6198998346437 average return: 157.7243156704679
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.703503231799508e-06 maximum return: 417.45263147721914 average return: 42.44156885845955
============================================================
Iteration 2/2

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 312 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (312000, 27)
Actions: (312000, 8)
Rewards-to-Go: (312000,)
Time-to-Go: (312000,)
Goal Vectors: (312000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 163.93791
AVERAGE OBTAINED REWARD PER EPISODE: 258.0975094599841
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0103, Val Loss = 0.0062
Best model found! Validation Loss: 0.0062
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0072, Val Loss = 0.0055
Best model found! Validation Loss: 0.0055
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0064, Val Loss = 0.0053
Best model found! Validation Loss: 0.0053
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0058, Val Loss = 0.0053
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0055, Val Loss = 0.0051
Best model found! Validation Loss: 0.0051
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0052, Val Loss = 0.0051
Best model found! Validation Loss: 0.0051
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0050, Val Loss = 0.0048
Best model found! Validation Loss: 0.0048
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0048, Val Loss = 0.0050
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0046, Val Loss = 0.0048
Best model found! Validation Loss: 0.0048
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0044, Val Loss = 0.0050
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0043, Val Loss = 0.0052
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0042, Val Loss = 0.0048
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0041, Val Loss = 0.0048
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0040, Val Loss = 0.0049
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0038, Val Loss = 0.0048
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0038, Val Loss = 0.0047
Best model found! Validation Loss: 0.0047
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0037, Val Loss = 0.0045
Best model found! Validation Loss: 0.0045
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0036, Val Loss = 0.0047
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0036, Val Loss = 0.0048
Epoch 20/100 [Train]: Starting...
Epoch 20/100 [Val  ]: Starting...
Epoch 20/100: Train Loss = 0.0035, Val Loss = 0.0046
Epoch 21/100 [Train]: Starting...
Epoch 21/100 [Val  ]: Starting...
Epoch 21/100: Train Loss = 0.0034, Val Loss = 0.0047
Epoch 22/100 [Train]: Starting...
Epoch 22/100 [Val  ]: Starting...
Epoch 22/100: Train Loss = 0.0034, Val Loss = 0.0047
Epoch 23/100 [Train]: Starting...
Epoch 23/100 [Val  ]: Starting...
Epoch 23/100: Train Loss = 0.0033, Val Loss = 0.0047
Epoch 24/100 [Train]: Starting...
Epoch 24/100 [Val  ]: Starting...
Epoch 24/100: Train Loss = 0.0032, Val Loss = 0.0046
Epoch 25/100 [Train]: Starting...
Epoch 25/100 [Val  ]: Starting...
Epoch 25/100: Train Loss = 0.0032, Val Loss = 0.0045
Best model found! Validation Loss: 0.0045
Epoch 26/100 [Train]: Starting...
Epoch 26/100 [Val  ]: Starting...
Epoch 26/100: Train Loss = 0.0031, Val Loss = 0.0047
Epoch 27/100 [Train]: Starting...
Epoch 27/100 [Val  ]: Starting...
Epoch 27/100: Train Loss = 0.0031, Val Loss = 0.0046
Epoch 28/100 [Train]: Starting...
Epoch 28/100 [Val  ]: Starting...
Epoch 28/100: Train Loss = 0.0030, Val Loss = 0.0045
Epoch 29/100 [Train]: Starting...
Epoch 29/100 [Val  ]: Starting...
Epoch 29/100: Train Loss = 0.0030, Val Loss = 0.0048
Epoch 30/100 [Train]: Starting...
Epoch 30/100 [Val  ]: Starting...
Epoch 30/100: Train Loss = 0.0030, Val Loss = 0.0044
Best model found! Validation Loss: 0.0044
Epoch 31/100 [Train]: Starting...
Epoch 31/100 [Val  ]: Starting...
Epoch 31/100: Train Loss = 0.0029, Val Loss = 0.0045
Epoch 32/100 [Train]: Starting...
Epoch 32/100 [Val  ]: Starting...
Epoch 32/100: Train Loss = 0.0029, Val Loss = 0.0044
Best model found! Validation Loss: 0.0044
Epoch 33/100 [Train]: Starting...
Epoch 33/100 [Val  ]: Starting...
Epoch 33/100: Train Loss = 0.0028, Val Loss = 0.0045
Epoch 34/100 [Train]: Starting...
Epoch 34/100 [Val  ]: Starting...
Epoch 34/100: Train Loss = 0.0028, Val Loss = 0.0046
Epoch 35/100 [Train]: Starting...
Epoch 35/100 [Val  ]: Starting...
Epoch 35/100: Train Loss = 0.0028, Val Loss = 0.0045
Epoch 36/100 [Train]: Starting...
Epoch 36/100 [Val  ]: Starting...
Epoch 36/100: Train Loss = 0.0027, Val Loss = 0.0045
Epoch 37/100 [Train]: Starting...
Epoch 37/100 [Val  ]: Starting...
Epoch 37/100: Train Loss = 0.0027, Val Loss = 0.0046
Epoch 38/100 [Train]: Starting...
Epoch 38/100 [Val  ]: Starting...
Epoch 38/100: Train Loss = 0.0027, Val Loss = 0.0046
Epoch 39/100 [Train]: Starting...
Epoch 39/100 [Val  ]: Starting...
Epoch 39/100: Train Loss = 0.0027, Val Loss = 0.0045
Epoch 40/100 [Train]: Starting...
Epoch 40/100 [Val  ]: Starting...
Epoch 40/100: Train Loss = 0.0026, Val Loss = 0.0046
Epoch 41/100 [Train]: Starting...
Epoch 41/100 [Val  ]: Starting...
Epoch 41/100: Train Loss = 0.0026, Val Loss = 0.0046
Epoch 42/100 [Train]: Starting...
Epoch 42/100 [Val  ]: Starting...
Epoch 42/100: Train Loss = 0.0026, Val Loss = 0.0045
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP2
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.000951324258240446 maximum return: 219.91349477604626 average return: 38.794952758614436
==================================================
Evaluating d_r: 50
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 6.051174618301455e-05 maximum return: 79.41553689845858 average return: 24.82346776217821
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.005797906892701775 maximum return: 102.03654727037203 average return: 40.849719059130244
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.8741530888931222e-06 maximum return: 158.60707821070937 average return: 41.43672534848652
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 6.534799119705415e-06 maximum return: 332.96048226659826 average return: 35.86376707753206
==================================================
Evaluating d_r: 250
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 0.21458592425653356 maximum return: 215.28448106443366 average return: 110.49576043791949
==================================================
Evaluating d_r: 300
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.1378551977621947e-05 maximum return: 312.44136199626064 average return: 100.37495951831043
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 3.857230332200255e-06 maximum return: 367.87609297424746 average return: 67.21763743446328
==================================================
Evaluating d_r: 400
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 7.2133406578851135e-06 maximum return: 286.93721419762574 average return: 70.9099487488231
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 2.0756609406739465e-06 maximum return: 466.9574359953913 average return: 47.11601276226321
==================================================
Evaluating d_r: 500
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 6.707563270894912e-05 maximum return: 508.34216774441796 average return: 139.53583164061834
==================================================
Evaluating d_r: 550
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 7.638049451860293e-07 maximum return: 533.4574029537835 average return: 242.55209940935507
==================================================
Evaluating d_r: 600
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.910706086443824e-07 maximum return: 703.056932190938 average return: 70.69887803807381
==================================================
Evaluating d_r: 650
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 0.00016268175019468705 maximum return: 682.7987196013332 average return: 207.9134318652767
==================================================
Evaluating d_r: 700
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.6035090533580156e-07 maximum return: 679.7087932313041 average return: 250.4254080475589
==================================================
Evaluating d_r: 750
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 4.871299803667795e-06 maximum return: 846.7572711375666 average return: 286.5050272085008
==================================================
Evaluating d_r: 800
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 5.29181283791665e-08 maximum return: 792.8417989439314 average return: 79.53617080209564
==================================================
Evaluating d_r: 850
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 8.987344962076671e-05 maximum return: 678.9387851504691 average return: 307.7727432328601
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.1806965320078481e-08 maximum return: 570.4687965585297 average return: 57.9359757665423
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 2.1808853656215823e-06 maximum return: 653.65638481593 average return: 66.07812407786109
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.6708369133175295e-06 maximum return: 727.3666542225756 average return: 142.4081801793078
Saved combined plot to condition5-ANTMAZE_BERT_MLP.png

============================================================
Final Success Rates per Model:
UDRLt_MLP0: 34.29%
UDRLt_MLP1: 29.52%
UDRLt_MLP2: 28.10%

###############################################################################
H치br칩k Cluster
Job 17928194 for user s5173019
Finished at: Sun Jun 15 19:23:34 CEST 2025

Job details:
============

Job ID                         : 17928194
Name                           : jobscript-gpu.sh
User                           : s5173019
Partition                      : gpumedium
Nodes                          : v100v2gpu13
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : COMPLETED  
Submit                         : 2025-06-15T14:05:32
Start                          : 2025-06-15T14:07:22
End                            : 2025-06-15T19:23:30
Reserved walltime              : 08:00:00
Used walltime                  : 05:16:08
Used CPU time                  : 05:12:46 (Efficiency: 12.37%)
% User (Computation)           : 99.84%
% System (I/O)                 :  0.16%
Total memory reserved          : 8000M
Maximum memory used            : 1.16G
Requested GPUs                 : 1
Allocated GPUs                 : v100=1
Max GPU utilization            : 30%
Max GPU memory used            : 582.00M

Acknowledgements:
=================

Please see this page for information about acknowledging H치br칩k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
