Evaluating base model: UDRLt_MLP0
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 3.221109285304018e-08 maximum return: 155.33835921821034 average return: 34.958814823520036
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.8048394006039236e-08 maximum return: 145.4154359552854 average return: 29.581355142476248
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.637666234958411e-07 maximum return: 159.0509229024095 average return: 30.572309209295224
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.003154315712836039 maximum return: 206.40650606721044 average return: 87.76853785741996
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.083461996949479e-05 maximum return: 227.15801648488923 average return: 43.91315180406911
==================================================
Evaluating d_r: 250
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 0.0021591610048165635 maximum return: 284.4016556549603 average return: 79.27718483492481
==================================================
Evaluating d_r: 300
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.0034706593618349705 maximum return: 323.61660628487965 average return: 87.25996210306454
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 5.571150786930578e-07 maximum return: 368.68529841986395 average return: 69.02885684903228
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 7.488186954067293e-09 maximum return: 391.7266295000349 average return: 144.55498450378283
==================================================
Evaluating d_r: 450
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 3.996548259279347e-06 maximum return: 468.3862114155882 average return: 252.4316081916657
==================================================
Evaluating d_r: 500
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.0013647757743683385 maximum return: 521.9060854999167 average return: 88.98987903975957
==================================================
Evaluating d_r: 550
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.392369595548757e-06 maximum return: 527.1566649234429 average return: 53.036712875994496
==================================================
Evaluating d_r: 600
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 1.7307885481321634e-06 maximum return: 604.765071469649 average return: 338.8612757494242
==================================================
Evaluating d_r: 650
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 0.009355278953952408 maximum return: 495.24227098025494 average return: 212.09490587700188
==================================================
Evaluating d_r: 700
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 4.5177457223626015e-09 maximum return: 695.8506681903553 average return: 189.9896901392537
==================================================
Evaluating d_r: 750
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 8.169159607509938e-05 maximum return: 729.9327036551516 average return: 214.27485558302132
==================================================
Evaluating d_r: 800
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 4.325668300318395e-05 maximum return: 773.8827969798454 average return: 219.29858833539774
==================================================
Evaluating d_r: 850
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 0.00044032686461191424 maximum return: 856.0816286435386 average return: 168.57344083640166
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 0.0011624499357140467 maximum return: 887.2511228089941 average return: 399.5812696756225
==================================================
Evaluating d_r: 950
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 8.390415583211528e-08 maximum return: 901.5127361403569 average return: 386.70536136379303
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 5.113697174388979e-07 maximum return: 556.4123718714425 average return: 58.1254053291605
============================================================
Iteration 1/5

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 312 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (312000, 27)
Actions: (312000, 8)
Rewards-to-Go: (312000,)
Time-to-Go: (312000,)
Goal Vectors: (312000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 150.11945
AVERAGE OBTAINED REWARD PER EPISODE: 231.2389017525385
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0213, Val Loss = 0.0133
Best model found! Validation Loss: 0.0133
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0146, Val Loss = 0.0116
Best model found! Validation Loss: 0.0116
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0128, Val Loss = 0.0112
Best model found! Validation Loss: 0.0112
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0117, Val Loss = 0.0104
Best model found! Validation Loss: 0.0104
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0109, Val Loss = 0.0101
Best model found! Validation Loss: 0.0101
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0103, Val Loss = 0.0102
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0098, Val Loss = 0.0099
Best model found! Validation Loss: 0.0099
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0094, Val Loss = 0.0098
Best model found! Validation Loss: 0.0098
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0090, Val Loss = 0.0100
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0086, Val Loss = 0.0098
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0084, Val Loss = 0.0093
Best model found! Validation Loss: 0.0093
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0081, Val Loss = 0.0091
Best model found! Validation Loss: 0.0091
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0078, Val Loss = 0.0092
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0076, Val Loss = 0.0099
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0075, Val Loss = 0.0092
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0073, Val Loss = 0.0090
Best model found! Validation Loss: 0.0090
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0071, Val Loss = 0.0089
Best model found! Validation Loss: 0.0089
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0069, Val Loss = 0.0090
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0068, Val Loss = 0.0088
Best model found! Validation Loss: 0.0088
Epoch 20/100 [Train]: Starting...
Epoch 20/100 [Val  ]: Starting...
Epoch 20/100: Train Loss = 0.0067, Val Loss = 0.0091
Epoch 21/100 [Train]: Starting...
Epoch 21/100 [Val  ]: Starting...
Epoch 21/100: Train Loss = 0.0066, Val Loss = 0.0090
Epoch 22/100 [Train]: Starting...
Epoch 22/100 [Val  ]: Starting...
Epoch 22/100: Train Loss = 0.0064, Val Loss = 0.0092
Epoch 23/100 [Train]: Starting...
Epoch 23/100 [Val  ]: Starting...
Epoch 23/100: Train Loss = 0.0064, Val Loss = 0.0090
Epoch 24/100 [Train]: Starting...
Epoch 24/100 [Val  ]: Starting...
Epoch 24/100: Train Loss = 0.0062, Val Loss = 0.0089
Epoch 25/100 [Train]: Starting...
Epoch 25/100 [Val  ]: Starting...
Epoch 25/100: Train Loss = 0.0061, Val Loss = 0.0088
Best model found! Validation Loss: 0.0088
Epoch 26/100 [Train]: Starting...
Epoch 26/100 [Val  ]: Starting...
Epoch 26/100: Train Loss = 0.0060, Val Loss = 0.0089
Epoch 27/100 [Train]: Starting...
Epoch 27/100 [Val  ]: Starting...
Epoch 27/100: Train Loss = 0.0059, Val Loss = 0.0087
Best model found! Validation Loss: 0.0087
Epoch 28/100 [Train]: Starting...
Epoch 28/100 [Val  ]: Starting...
Epoch 28/100: Train Loss = 0.0058, Val Loss = 0.0088
Epoch 29/100 [Train]: Starting...
Epoch 29/100 [Val  ]: Starting...
Epoch 29/100: Train Loss = 0.0057, Val Loss = 0.0088
Epoch 30/100 [Train]: Starting...
Epoch 30/100 [Val  ]: Starting...
Epoch 30/100: Train Loss = 0.0056, Val Loss = 0.0089
Epoch 31/100 [Train]: Starting...
Epoch 31/100 [Val  ]: Starting...
Epoch 31/100: Train Loss = 0.0055, Val Loss = 0.0086
Best model found! Validation Loss: 0.0086
Epoch 32/100 [Train]: Starting...
Epoch 32/100 [Val  ]: Starting...
Epoch 32/100: Train Loss = 0.0054, Val Loss = 0.0086
Epoch 33/100 [Train]: Starting...
Epoch 33/100 [Val  ]: Starting...
Epoch 33/100: Train Loss = 0.0054, Val Loss = 0.0090
Epoch 34/100 [Train]: Starting...
Epoch 34/100 [Val  ]: Starting...
Epoch 34/100: Train Loss = 0.0053, Val Loss = 0.0087
Epoch 35/100 [Train]: Starting...
Epoch 35/100 [Val  ]: Starting...
Epoch 35/100: Train Loss = 0.0053, Val Loss = 0.0086
Best model found! Validation Loss: 0.0086
Epoch 36/100 [Train]: Starting...
Epoch 36/100 [Val  ]: Starting...
Epoch 36/100: Train Loss = 0.0052, Val Loss = 0.0087
Epoch 37/100 [Train]: Starting...
Epoch 37/100 [Val  ]: Starting...
Epoch 37/100: Train Loss = 0.0051, Val Loss = 0.0084
Best model found! Validation Loss: 0.0084
Epoch 38/100 [Train]: Starting...
Epoch 38/100 [Val  ]: Starting...
Epoch 38/100: Train Loss = 0.0050, Val Loss = 0.0087
Epoch 39/100 [Train]: Starting...
Epoch 39/100 [Val  ]: Starting...
Epoch 39/100: Train Loss = 0.0049, Val Loss = 0.0088
Epoch 40/100 [Train]: Starting...
Epoch 40/100 [Val  ]: Starting...
Epoch 40/100: Train Loss = 0.0049, Val Loss = 0.0084
Best model found! Validation Loss: 0.0084
Epoch 41/100 [Train]: Starting...
Epoch 41/100 [Val  ]: Starting...
Epoch 41/100: Train Loss = 0.0049, Val Loss = 0.0087
Epoch 42/100 [Train]: Starting...
Epoch 42/100 [Val  ]: Starting...
Epoch 42/100: Train Loss = 0.0048, Val Loss = 0.0085
Epoch 43/100 [Train]: Starting...
Epoch 43/100 [Val  ]: Starting...
Epoch 43/100: Train Loss = 0.0047, Val Loss = 0.0090
Epoch 44/100 [Train]: Starting...
Epoch 44/100 [Val  ]: Starting...
Epoch 44/100: Train Loss = 0.0047, Val Loss = 0.0084
Epoch 45/100 [Train]: Starting...
Epoch 45/100 [Val  ]: Starting...
Epoch 45/100: Train Loss = 0.0046, Val Loss = 0.0087
Epoch 46/100 [Train]: Starting...
Epoch 46/100 [Val  ]: Starting...
Epoch 46/100: Train Loss = 0.0045, Val Loss = 0.0086
Epoch 47/100 [Train]: Starting...
Epoch 47/100 [Val  ]: Starting...
Epoch 47/100: Train Loss = 0.0045, Val Loss = 0.0087
Epoch 48/100 [Train]: Starting...
Epoch 48/100 [Val  ]: Starting...
Epoch 48/100: Train Loss = 0.0044, Val Loss = 0.0086
Epoch 49/100 [Train]: Starting...
Epoch 49/100 [Val  ]: Starting...
Epoch 49/100: Train Loss = 0.0044, Val Loss = 0.0085
Epoch 50/100 [Train]: Starting...
Epoch 50/100 [Val  ]: Starting...
Epoch 50/100: Train Loss = 0.0043, Val Loss = 0.0085
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP1
==================================================
Evaluating d_r: 0
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 0.07483836515386509 maximum return: 115.93808660008932 average return: 40.70299432615893
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00016910432141251105 maximum return: 72.01407840426761 average return: 14.705515235023332
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 7.277228689589286e-07 maximum return: 126.50835445324229 average return: 23.09830495336821
==================================================
Evaluating d_r: 150
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.00013481425716837186 maximum return: 164.96268489758384 average return: 71.39287119340152
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.9349991361171425e-05 maximum return: 323.9941312746543 average return: 68.5244266774542
==================================================
Evaluating d_r: 250
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.5313516034460954e-07 maximum return: 260.35723618173245 average return: 89.40219821775838
==================================================
Evaluating d_r: 300
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 2.0315571399832585e-06 maximum return: 242.5355069039281 average return: 28.45330152289913
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 2.489094522944636e-08 maximum return: 386.22918417484715 average return: 72.3083794168981
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 0.003361969955424984 maximum return: 362.14012195794527 average return: 70.60777778572336
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.0015958080830241648 maximum return: 431.76534142427226 average return: 54.03104016497556
==================================================
Evaluating d_r: 500
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.005581980091816774 maximum return: 482.50781079736885 average return: 135.19226030211172
==================================================
Evaluating d_r: 550
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.0009737980948002297 maximum return: 565.6460874248496 average return: 249.26606999282376
==================================================
Evaluating d_r: 600
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 2.4792579616520777e-06 maximum return: 621.5515624265362 average return: 238.6369509664236
==================================================
Evaluating d_r: 650
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 0.017225485463782463 maximum return: 643.2904739209446 average return: 373.814543741731
==================================================
Evaluating d_r: 700
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.1632881337197407e-05 maximum return: 708.1565948195301 average return: 139.91091114670454
==================================================
Evaluating d_r: 750
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 1.4111852671394608e-06 maximum return: 786.9008520450192 average return: 271.83990610175715
==================================================
Evaluating d_r: 800
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00044027007740894843 maximum return: 853.0455149014869 average return: 155.27329609028772
==================================================
Evaluating d_r: 850
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.783029064294165e-07 maximum return: 806.3602743273497 average return: 300.3111848980321
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.7497781587208956e-07 maximum return: 894.3010666002845 average return: 167.20192556575194
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 2.768004524627991e-07 maximum return: 841.9877638424382 average return: 172.70092731607565
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.801499075361722e-07 maximum return: 15.680129296799715 average return: 1.8223978953838824
============================================================
Iteration 2/5

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 306 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (306000, 27)
Actions: (306000, 8)
Rewards-to-Go: (306000,)
Time-to-Go: (306000,)
Goal Vectors: (306000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 121.623566
AVERAGE OBTAINED REWARD PER EPISODE: 191.8251293886258
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0107, Val Loss = 0.0071
Best model found! Validation Loss: 0.0071
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0071, Val Loss = 0.0061
Best model found! Validation Loss: 0.0061
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0062, Val Loss = 0.0057
Best model found! Validation Loss: 0.0057
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0057, Val Loss = 0.0055
Best model found! Validation Loss: 0.0055
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0052, Val Loss = 0.0051
Best model found! Validation Loss: 0.0051
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0049, Val Loss = 0.0053
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0047, Val Loss = 0.0050
Best model found! Validation Loss: 0.0050
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0045, Val Loss = 0.0048
Best model found! Validation Loss: 0.0048
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0043, Val Loss = 0.0049
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0041, Val Loss = 0.0049
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0040, Val Loss = 0.0048
Best model found! Validation Loss: 0.0048
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0039, Val Loss = 0.0045
Best model found! Validation Loss: 0.0045
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0037, Val Loss = 0.0048
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0036, Val Loss = 0.0047
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0035, Val Loss = 0.0050
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0035, Val Loss = 0.0047
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0034, Val Loss = 0.0050
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0033, Val Loss = 0.0046
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0033, Val Loss = 0.0049
Epoch 20/100 [Train]: Starting...
Epoch 20/100 [Val  ]: Starting...
Epoch 20/100: Train Loss = 0.0032, Val Loss = 0.0047
Epoch 21/100 [Train]: Starting...
Epoch 21/100 [Val  ]: Starting...
Epoch 21/100: Train Loss = 0.0031, Val Loss = 0.0047
Epoch 22/100 [Train]: Starting...
Epoch 22/100 [Val  ]: Starting...
Epoch 22/100: Train Loss = 0.0031, Val Loss = 0.0046
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP2
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.250228556457738e-09 maximum return: 406.3632534600398 average return: 69.02105460406094
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.544056624570923e-06 maximum return: 72.48890969239444 average return: 8.00606825894663
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 1.732795711800739e-07 maximum return: 105.41900982400213 average return: 10.730967347795605
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.000637616194401759 maximum return: 305.53711799890334 average return: 43.43142467553331
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.88639025074518e-06 maximum return: 183.82851982339463 average return: 28.450554014586395
==================================================
Evaluating d_r: 250
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.1421551249973175e-05 maximum return: 257.71826551559536 average return: 44.519024889197674
==================================================
Evaluating d_r: 300
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.1405771344999415e-05 maximum return: 244.19405719582696 average return: 27.38523091762413
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 4.47628361272155e-08 maximum return: 373.52474157813043 average return: 37.871469662928625
==================================================
Evaluating d_r: 400
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 1.5260140653395438e-05 maximum return: 389.8727261317212 average return: 129.6391714495886
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.0013922013030942888 maximum return: 380.57416799784056 average return: 52.894492314783825
==================================================
Evaluating d_r: 500
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 6.713740073950344e-06 maximum return: 536.6210408720759 average return: 218.5736535686292
==================================================
Evaluating d_r: 550
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00535352269174459 maximum return: 515.5102491811004 average return: 180.7484400438429
==================================================
Evaluating d_r: 600
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 2.340244179846386e-08 maximum return: 599.2145254950059 average return: 171.13911780884735
==================================================
Evaluating d_r: 650
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.18327252212668133 maximum return: 693.0542662955694 average return: 293.52626780078856
==================================================
Evaluating d_r: 700
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.0003903869938659249 maximum return: 754.59217183917 average return: 236.90198221171903
==================================================
Evaluating d_r: 750
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.5644263736688997e-05 maximum return: 803.193570579015 average return: 338.3981262767076
==================================================
Evaluating d_r: 800
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 8.42563661238881e-09 maximum return: 610.1238010183346 average return: 155.43060918370048
==================================================
Evaluating d_r: 850
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 1.005720779100512e-07 maximum return: 900.5776303228314 average return: 249.98592226261934
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.156537081146817e-08 maximum return: 671.1918570576462 average return: 129.36457044434098
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 1.9126413428166706e-08 maximum return: 723.4890145154527 average return: 183.05024706172125
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 1.1321812600303858e-08 maximum return: 543.9108759510707 average return: 54.53231588313489
============================================================
Iteration 3/5

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 297 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (297000, 27)
Actions: (297000, 8)
Rewards-to-Go: (297000,)
Time-to-Go: (297000,)
Goal Vectors: (297000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 100.308716
AVERAGE OBTAINED REWARD PER EPISODE: 162.5113980488488
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0065, Val Loss = 0.0041
Best model found! Validation Loss: 0.0041
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0042, Val Loss = 0.0036
Best model found! Validation Loss: 0.0036
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0037, Val Loss = 0.0033
Best model found! Validation Loss: 0.0033
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0033, Val Loss = 0.0032
Best model found! Validation Loss: 0.0032
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0031, Val Loss = 0.0032
Best model found! Validation Loss: 0.0032
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0029, Val Loss = 0.0031
Best model found! Validation Loss: 0.0031
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0028, Val Loss = 0.0032
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0027, Val Loss = 0.0031
Best model found! Validation Loss: 0.0031
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0025, Val Loss = 0.0032
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0024, Val Loss = 0.0030
Best model found! Validation Loss: 0.0030
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0024, Val Loss = 0.0030
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0023, Val Loss = 0.0030
Best model found! Validation Loss: 0.0030
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0022, Val Loss = 0.0034
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0022, Val Loss = 0.0029
Best model found! Validation Loss: 0.0029
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0021, Val Loss = 0.0028
Best model found! Validation Loss: 0.0028
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0021, Val Loss = 0.0029
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0020, Val Loss = 0.0032
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0020, Val Loss = 0.0030
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0019, Val Loss = 0.0028
Best model found! Validation Loss: 0.0028
Epoch 20/100 [Train]: Starting...
Epoch 20/100 [Val  ]: Starting...
Epoch 20/100: Train Loss = 0.0019, Val Loss = 0.0029
Epoch 21/100 [Train]: Starting...
Epoch 21/100 [Val  ]: Starting...
Epoch 21/100: Train Loss = 0.0019, Val Loss = 0.0028
Epoch 22/100 [Train]: Starting...
Epoch 22/100 [Val  ]: Starting...
Epoch 22/100: Train Loss = 0.0018, Val Loss = 0.0028
Epoch 23/100 [Train]: Starting...
Epoch 23/100 [Val  ]: Starting...
Epoch 23/100: Train Loss = 0.0018, Val Loss = 0.0028
Epoch 24/100 [Train]: Starting...
Epoch 24/100 [Val  ]: Starting...
Epoch 24/100: Train Loss = 0.0017, Val Loss = 0.0028
Epoch 25/100 [Train]: Starting...
Epoch 25/100 [Val  ]: Starting...
Epoch 25/100: Train Loss = 0.0017, Val Loss = 0.0029
Epoch 26/100 [Train]: Starting...
Epoch 26/100 [Val  ]: Starting...
Epoch 26/100: Train Loss = 0.0017, Val Loss = 0.0027
Best model found! Validation Loss: 0.0027
Epoch 27/100 [Train]: Starting...
Epoch 27/100 [Val  ]: Starting...
Epoch 27/100: Train Loss = 0.0017, Val Loss = 0.0031
Epoch 28/100 [Train]: Starting...
Epoch 28/100 [Val  ]: Starting...
Epoch 28/100: Train Loss = 0.0017, Val Loss = 0.0029
Epoch 29/100 [Train]: Starting...
Epoch 29/100 [Val  ]: Starting...
Epoch 29/100: Train Loss = 0.0016, Val Loss = 0.0029
Epoch 30/100 [Train]: Starting...
Epoch 30/100 [Val  ]: Starting...
Epoch 30/100: Train Loss = 0.0016, Val Loss = 0.0029
Epoch 31/100 [Train]: Starting...
Epoch 31/100 [Val  ]: Starting...
Epoch 31/100: Train Loss = 0.0016, Val Loss = 0.0028
Epoch 32/100 [Train]: Starting...
Epoch 32/100 [Val  ]: Starting...
Epoch 32/100: Train Loss = 0.0016, Val Loss = 0.0028
Epoch 33/100 [Train]: Starting...
Epoch 33/100 [Val  ]: Starting...
Epoch 33/100: Train Loss = 0.0015, Val Loss = 0.0029
Epoch 34/100 [Train]: Starting...
Epoch 34/100 [Val  ]: Starting...
Epoch 34/100: Train Loss = 0.0015, Val Loss = 0.0029
Epoch 35/100 [Train]: Starting...
Epoch 35/100 [Val  ]: Starting...
Epoch 35/100: Train Loss = 0.0015, Val Loss = 0.0029
Epoch 36/100 [Train]: Starting...
Epoch 36/100 [Val  ]: Starting...
Epoch 36/100: Train Loss = 0.0015, Val Loss = 0.0028
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP3
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 5.647535072012674e-05 maximum return: 274.9692646984281 average return: 39.46952151440107
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 9.650522560157305e-05 maximum return: 93.34221777300557 average return: 11.440109781619434
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 8.473938705270579e-07 maximum return: 70.62735115091324 average return: 7.724096193716022
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 2.179491587702255e-06 maximum return: 182.19760842220558 average return: 38.74611886355886
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.2528621283081727e-06 maximum return: 348.16642397490244 average return: 71.08218894759585
==================================================
Evaluating d_r: 250
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 8.980796631732102e-09 maximum return: 153.68806820546405 average return: 31.38414971140467
==================================================
Evaluating d_r: 300
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.275021795010797e-08 maximum return: 330.1151439563063 average return: 71.62071805591924
==================================================
Evaluating d_r: 350
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 2.0279641530430043e-05 maximum return: 362.1546539880108 average return: 95.72680529726941
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 2.646507244841409e-07 maximum return: 390.70927463983315 average return: 39.1324575031339
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 1.728585468632745e-06 maximum return: 494.8157647848755 average return: 198.94968347557116
==================================================
Evaluating d_r: 500
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.009647826172270772 maximum return: 345.6579595843339 average return: 88.34415994046493
==================================================
Evaluating d_r: 550
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.340083510542695e-06 maximum return: 281.140929264841 average return: 55.11138727501045
==================================================
Evaluating d_r: 600
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 2.6512560459983747e-05 maximum return: 721.9592189158247 average return: 334.33399834181586
==================================================
Evaluating d_r: 650
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00016875681082419318 maximum return: 551.2881952455087 average return: 55.481242373537484
==================================================
Evaluating d_r: 700
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 6.896993023697806e-06 maximum return: 770.8382313895199 average return: 447.55937429527495
==================================================
Evaluating d_r: 750
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 1.2160015543537712e-07 maximum return: 662.1368105486088 average return: 215.2821771230599
==================================================
Evaluating d_r: 800
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 7.236693634088158e-08 maximum return: 867.3825790719917 average return: 295.4651016597007
==================================================
Evaluating d_r: 850
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00010250831420083876 maximum return: 642.3990409563161 average return: 167.95914356145514
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.633817119544609e-06 maximum return: 693.7163399950009 average return: 96.13213915484762
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 2.797024291449574e-07 maximum return: 362.5594085357535 average return: 62.41273331908809
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.00018385620370050073 maximum return: 484.46858470355465 average return: 93.64510563404305
============================================================
Iteration 4/5

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 302 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (302000, 27)
Actions: (302000, 8)
Rewards-to-Go: (302000,)
Time-to-Go: (302000,)
Goal Vectors: (302000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 99.11405
AVERAGE OBTAINED REWARD PER EPISODE: 165.01556880200292
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0047, Val Loss = 0.0029
Best model found! Validation Loss: 0.0029
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0030, Val Loss = 0.0027
Best model found! Validation Loss: 0.0027
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0026, Val Loss = 0.0026
Best model found! Validation Loss: 0.0026
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0024, Val Loss = 0.0026
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0022, Val Loss = 0.0024
Best model found! Validation Loss: 0.0024
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0020, Val Loss = 0.0023
Best model found! Validation Loss: 0.0023
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0019, Val Loss = 0.0022
Best model found! Validation Loss: 0.0022
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0018, Val Loss = 0.0023
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0018, Val Loss = 0.0022
Best model found! Validation Loss: 0.0022
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0017, Val Loss = 0.0021
Best model found! Validation Loss: 0.0021
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0016, Val Loss = 0.0022
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0016, Val Loss = 0.0022
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0015, Val Loss = 0.0022
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0015, Val Loss = 0.0021
Best model found! Validation Loss: 0.0021
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0014, Val Loss = 0.0021
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0014, Val Loss = 0.0021
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0014, Val Loss = 0.0021
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0014, Val Loss = 0.0020
Best model found! Validation Loss: 0.0020
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0013, Val Loss = 0.0021
Epoch 20/100 [Train]: Starting...
Epoch 20/100 [Val  ]: Starting...
Epoch 20/100: Train Loss = 0.0013, Val Loss = 0.0021
Epoch 21/100 [Train]: Starting...
Epoch 21/100 [Val  ]: Starting...
Epoch 21/100: Train Loss = 0.0013, Val Loss = 0.0020
Best model found! Validation Loss: 0.0020
Epoch 22/100 [Train]: Starting...
Epoch 22/100 [Val  ]: Starting...
Epoch 22/100: Train Loss = 0.0012, Val Loss = 0.0021
Epoch 23/100 [Train]: Starting...
Epoch 23/100 [Val  ]: Starting...
Epoch 23/100: Train Loss = 0.0012, Val Loss = 0.0021
Epoch 24/100 [Train]: Starting...
Epoch 24/100 [Val  ]: Starting...
Epoch 24/100: Train Loss = 0.0012, Val Loss = 0.0020
Best model found! Validation Loss: 0.0020
Epoch 25/100 [Train]: Starting...
Epoch 25/100 [Val  ]: Starting...
Epoch 25/100: Train Loss = 0.0012, Val Loss = 0.0020
Epoch 26/100 [Train]: Starting...
Epoch 26/100 [Val  ]: Starting...
Epoch 26/100: Train Loss = 0.0012, Val Loss = 0.0021
Epoch 27/100 [Train]: Starting...
Epoch 27/100 [Val  ]: Starting...
Epoch 27/100: Train Loss = 0.0011, Val Loss = 0.0020
Epoch 28/100 [Train]: Starting...
Epoch 28/100 [Val  ]: Starting...
Epoch 28/100: Train Loss = 0.0011, Val Loss = 0.0021
Epoch 29/100 [Train]: Starting...
Epoch 29/100 [Val  ]: Starting...
Epoch 29/100: Train Loss = 0.0011, Val Loss = 0.0020
Epoch 30/100 [Train]: Starting...
Epoch 30/100 [Val  ]: Starting...
Epoch 30/100: Train Loss = 0.0011, Val Loss = 0.0020
Epoch 31/100 [Train]: Starting...
Epoch 31/100 [Val  ]: Starting...
Epoch 31/100: Train Loss = 0.0011, Val Loss = 0.0020
Best model found! Validation Loss: 0.0020
Epoch 32/100 [Train]: Starting...
Epoch 32/100 [Val  ]: Starting...
Epoch 32/100: Train Loss = 0.0011, Val Loss = 0.0020
Epoch 33/100 [Train]: Starting...
Epoch 33/100 [Val  ]: Starting...
Epoch 33/100: Train Loss = 0.0010, Val Loss = 0.0020
Epoch 34/100 [Train]: Starting...
Epoch 34/100 [Val  ]: Starting...
Epoch 34/100: Train Loss = 0.0010, Val Loss = 0.0020
Epoch 35/100 [Train]: Starting...
Epoch 35/100 [Val  ]: Starting...
Epoch 35/100: Train Loss = 0.0010, Val Loss = 0.0021
Epoch 36/100 [Train]: Starting...
Epoch 36/100 [Val  ]: Starting...
Epoch 36/100: Train Loss = 0.0010, Val Loss = 0.0020
Epoch 37/100 [Train]: Starting...
Epoch 37/100 [Val  ]: Starting...
Epoch 37/100: Train Loss = 0.0010, Val Loss = 0.0019
Best model found! Validation Loss: 0.0019
Epoch 38/100 [Train]: Starting...
Epoch 38/100 [Val  ]: Starting...
Epoch 38/100: Train Loss = 0.0010, Val Loss = 0.0020
Epoch 39/100 [Train]: Starting...
Epoch 39/100 [Val  ]: Starting...
Epoch 39/100: Train Loss = 0.0010, Val Loss = 0.0020
Epoch 40/100 [Train]: Starting...
Epoch 40/100 [Val  ]: Starting...
Epoch 40/100: Train Loss = 0.0009, Val Loss = 0.0021
Epoch 41/100 [Train]: Starting...
Epoch 41/100 [Val  ]: Starting...
Epoch 41/100: Train Loss = 0.0009, Val Loss = 0.0021
Epoch 42/100 [Train]: Starting...
Epoch 42/100 [Val  ]: Starting...
Epoch 42/100: Train Loss = 0.0009, Val Loss = 0.0020
Epoch 43/100 [Train]: Starting...
Epoch 43/100 [Val  ]: Starting...
Epoch 43/100: Train Loss = 0.0009, Val Loss = 0.0020
Epoch 44/100 [Train]: Starting...
Epoch 44/100 [Val  ]: Starting...
Epoch 44/100: Train Loss = 0.0009, Val Loss = 0.0020
Epoch 45/100 [Train]: Starting...
Epoch 45/100 [Val  ]: Starting...
Epoch 45/100: Train Loss = 0.0009, Val Loss = 0.0020
Epoch 46/100 [Train]: Starting...
Epoch 46/100 [Val  ]: Starting...
Epoch 46/100: Train Loss = 0.0009, Val Loss = 0.0020
Epoch 47/100 [Train]: Starting...
Epoch 47/100 [Val  ]: Starting...
Epoch 47/100: Train Loss = 0.0009, Val Loss = 0.0020
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP4
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 9.73746709406367e-08 maximum return: 35.42956566873605 average return: 4.057875837123397
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.02140986422683686 maximum return: 258.5067607615577 average return: 33.15850998380925
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 8.580857192082472e-09 maximum return: 143.25423086533365 average return: 17.72372452620773
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 2.4292309495500348e-05 maximum return: 110.94345671546567 average return: 11.414111068508813
==================================================
Evaluating d_r: 200
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 2.1711647217030384e-09 maximum return: 353.1208619388546 average return: 80.52482198851467
==================================================
Evaluating d_r: 250
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.0005535614089618731 maximum return: 285.9645507723443 average return: 57.605315690912974
==================================================
Evaluating d_r: 300
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 2.903397993424392e-07 maximum return: 380.76596261842724 average return: 176.9362299994813
==================================================
Evaluating d_r: 350
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.044122576740835e-07 maximum return: 716.0158923733652 average return: 129.69994131345553
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 4.21464152065841e-06 maximum return: 1.1424004529297438 average return: 0.2027775471809034
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.0006976648926589429 maximum return: 396.18974588992177 average return: 140.35711541148046
==================================================
Evaluating d_r: 500
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
goal reached!
minimum return: 6.457784362881352e-09 maximum return: 598.9390514307684 average return: 176.7560973477924
==================================================
Evaluating d_r: 550
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 6.216495140114461e-08 maximum return: 575.3927772302059 average return: 114.53698860259396
==================================================
Evaluating d_r: 600
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
goal reached!
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 8.883670452805774e-06 maximum return: 677.6983299750401 average return: 248.1512177487888
==================================================
Evaluating d_r: 650
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 7.983526920382224e-05 maximum return: 538.9750929084961 average return: 213.74244356045628
==================================================
Evaluating d_r: 700
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 0.005378819671740954 maximum return: 629.075183470274 average return: 243.23747129756552
==================================================
Evaluating d_r: 750
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 0.015122985435632792 maximum return: 751.0168821884807 average return: 278.6089152810017
==================================================
Evaluating d_r: 800
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 0.0023396227346588567 maximum return: 554.2167713511318 average return: 96.6909847718271
==================================================
Evaluating d_r: 850
Episode: 0
goal reached!
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 9.945909093328167e-06 maximum return: 586.1889136187738 average return: 186.03669328934228
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.58458869504231e-06 maximum return: 394.0546454169653 average return: 39.494165052022446
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.6260211685214994e-07 maximum return: 373.16090430239416 average return: 39.466556989407295
==================================================
Evaluating d_r: 1000
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.1475099484945378e-06 maximum return: 7.241559686830352 average return: 1.3214909444690872
============================================================
Iteration 5/5

Collecting data for d_r = 0 with d_h = 1000.0...

Collecting data for d_r = 50 with d_h = 1000.0...

Collecting data for d_r = 100 with d_h = 1000.0...

Collecting data for d_r = 150 with d_h = 1000.0...

Collecting data for d_r = 200 with d_h = 1000.0...

Collecting data for d_r = 250 with d_h = 1000.0...

Collecting data for d_r = 300 with d_h = 1000.0...

Collecting data for d_r = 350 with d_h = 1000.0...

Collecting data for d_r = 400 with d_h = 1000.0...

Collecting data for d_r = 450 with d_h = 1000.0...

Collecting data for d_r = 500 with d_h = 1000.0...

Collecting data for d_r = 550 with d_h = 1000.0...

Collecting data for d_r = 600 with d_h = 1000.0...

Collecting data for d_r = 650 with d_h = 1000.0...

Collecting data for d_r = 700 with d_h = 1000.0...

Collecting data for d_r = 750 with d_h = 1000.0...

Collecting data for d_r = 800 with d_h = 1000.0...

Collecting data for d_r = 850 with d_h = 1000.0...

Collecting data for d_r = 900 with d_h = 1000.0...

Collecting data for d_r = 950 with d_h = 1000.0...

Collecting data for d_r = 1000 with d_h = 1000.0...

Collected 297 episodes. Now concatenating and saving...
Shapes of saved datasets:
Observations: (297000, 27)
Actions: (297000, 8)
Rewards-to-Go: (297000,)
Time-to-Go: (297000,)
Goal Vectors: (297000, 2)
stats: ================================================
AVERAGE REWARD TO GO: 90.05775
AVERAGE OBTAINED REWARD PER EPISODE: 150.96010717409325
==================================================
Data processing complete. Final dataset saved to antmaze_rollout_current_dataset.hdf5

Running grid search with BATCH_SIZE=16, LEARNING_RATE=5e-05, EPOCHS=100
Epoch 1/100 [Train]: Starting...
Epoch 1/100 [Val  ]: Starting...
Epoch 1/100: Train Loss = 0.0031, Val Loss = 0.0019
Best model found! Validation Loss: 0.0019
Epoch 2/100 [Train]: Starting...
Epoch 2/100 [Val  ]: Starting...
Epoch 2/100: Train Loss = 0.0019, Val Loss = 0.0016
Best model found! Validation Loss: 0.0016
Epoch 3/100 [Train]: Starting...
Epoch 3/100 [Val  ]: Starting...
Epoch 3/100: Train Loss = 0.0016, Val Loss = 0.0016
Epoch 4/100 [Train]: Starting...
Epoch 4/100 [Val  ]: Starting...
Epoch 4/100: Train Loss = 0.0015, Val Loss = 0.0015
Best model found! Validation Loss: 0.0015
Epoch 5/100 [Train]: Starting...
Epoch 5/100 [Val  ]: Starting...
Epoch 5/100: Train Loss = 0.0013, Val Loss = 0.0014
Best model found! Validation Loss: 0.0014
Epoch 6/100 [Train]: Starting...
Epoch 6/100 [Val  ]: Starting...
Epoch 6/100: Train Loss = 0.0013, Val Loss = 0.0014
Best model found! Validation Loss: 0.0014
Epoch 7/100 [Train]: Starting...
Epoch 7/100 [Val  ]: Starting...
Epoch 7/100: Train Loss = 0.0012, Val Loss = 0.0014
Epoch 8/100 [Train]: Starting...
Epoch 8/100 [Val  ]: Starting...
Epoch 8/100: Train Loss = 0.0011, Val Loss = 0.0015
Epoch 9/100 [Train]: Starting...
Epoch 9/100 [Val  ]: Starting...
Epoch 9/100: Train Loss = 0.0011, Val Loss = 0.0013
Best model found! Validation Loss: 0.0013
Epoch 10/100 [Train]: Starting...
Epoch 10/100 [Val  ]: Starting...
Epoch 10/100: Train Loss = 0.0010, Val Loss = 0.0013
Epoch 11/100 [Train]: Starting...
Epoch 11/100 [Val  ]: Starting...
Epoch 11/100: Train Loss = 0.0010, Val Loss = 0.0014
Epoch 12/100 [Train]: Starting...
Epoch 12/100 [Val  ]: Starting...
Epoch 12/100: Train Loss = 0.0010, Val Loss = 0.0013
Epoch 13/100 [Train]: Starting...
Epoch 13/100 [Val  ]: Starting...
Epoch 13/100: Train Loss = 0.0010, Val Loss = 0.0013
Epoch 14/100 [Train]: Starting...
Epoch 14/100 [Val  ]: Starting...
Epoch 14/100: Train Loss = 0.0009, Val Loss = 0.0013
Epoch 15/100 [Train]: Starting...
Epoch 15/100 [Val  ]: Starting...
Epoch 15/100: Train Loss = 0.0009, Val Loss = 0.0014
Epoch 16/100 [Train]: Starting...
Epoch 16/100 [Val  ]: Starting...
Epoch 16/100: Train Loss = 0.0009, Val Loss = 0.0014
Epoch 17/100 [Train]: Starting...
Epoch 17/100 [Val  ]: Starting...
Epoch 17/100: Train Loss = 0.0009, Val Loss = 0.0014
Epoch 18/100 [Train]: Starting...
Epoch 18/100 [Val  ]: Starting...
Epoch 18/100: Train Loss = 0.0008, Val Loss = 0.0013
Epoch 19/100 [Train]: Starting...
Epoch 19/100 [Val  ]: Starting...
Epoch 19/100: Train Loss = 0.0008, Val Loss = 0.0013
Early stopping.
Model for this configuration saved to antmazeMERGED_tiny-18-512.pth

Grid Search Complete.
Evaluating model: UDRLt_MLP5
==================================================
Evaluating d_r: 0
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
goal reached!
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.1620893434473232e-07 maximum return: 54.522510534516385 average return: 9.029994788673958
==================================================
Evaluating d_r: 50
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 5.830376703370283e-05 maximum return: 151.36611058664727 average return: 16.30242251032314
==================================================
Evaluating d_r: 100
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
Episode: 9
minimum return: 3.28196541831694e-05 maximum return: 91.80545760178417 average return: 18.598348065545622
==================================================
Evaluating d_r: 150
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 0.002660334599132627 maximum return: 31.80459780079332 average return: 6.981627276945728
==================================================
Evaluating d_r: 200
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 1.197393265323625e-05 maximum return: 237.83309990442115 average return: 41.12628215787265
==================================================
Evaluating d_r: 250
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.509000743183615e-07 maximum return: 199.41375222552338 average return: 40.502951702043724
==================================================
Evaluating d_r: 300
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.1518297198206805e-08 maximum return: 751.055615124154 average return: 131.86032370328059
==================================================
Evaluating d_r: 350
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 0.005847507978745294 maximum return: 364.57896381933574 average return: 176.88182167848854
==================================================
Evaluating d_r: 400
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 8.31855836929738e-06 maximum return: 586.9442038462583 average return: 174.30148528655585
==================================================
Evaluating d_r: 450
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 7.1853640906557626e-06 maximum return: 610.2898349391123 average return: 137.15034162011625
==================================================
Evaluating d_r: 500
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
goal reached!
Episode: 4
goal reached!
Episode: 5
goal reached!
Episode: 6
goal reached!
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 7.651459403969591e-05 maximum return: 606.6050404707636 average return: 265.42898472181093
==================================================
Evaluating d_r: 550
Episode: 0
goal reached!
Episode: 1
Episode: 2
goal reached!
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 7.116992354901683e-07 maximum return: 762.9780206822867 average return: 249.03408921766436
==================================================
Evaluating d_r: 600
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 1.0777115241810161e-05 maximum return: 464.65782522397126 average return: 94.36013483738218
==================================================
Evaluating d_r: 650
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
goal reached!
Episode: 9
minimum return: 2.8145574762981037e-05 maximum return: 303.76917165108074 average return: 45.65496302386525
==================================================
Evaluating d_r: 700
Episode: 0
Episode: 1
goal reached!
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
goal reached!
minimum return: 2.7677914839191783e-06 maximum return: 362.76605677200314 average return: 81.83748496242644
==================================================
Evaluating d_r: 750
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
goal reached!
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 2.444495926563571e-06 maximum return: 711.6720066682749 average return: 256.4757927817415
==================================================
Evaluating d_r: 800
Episode: 0
Episode: 1
Episode: 2
goal reached!
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
goal reached!
Episode: 9
minimum return: 4.3494190824870224e-08 maximum return: 555.8613357025121 average return: 188.6693311801527
==================================================
Evaluating d_r: 850
Episode: 0
Episode: 1
Episode: 2
Episode: 3
goal reached!
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.0440820677253138e-08 maximum return: 418.10849973262174 average return: 87.16176748674573
==================================================
Evaluating d_r: 900
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
goal reached!
minimum return: 3.848666132596186e-06 maximum return: 624.3120426893652 average return: 106.58405327452226
==================================================
Evaluating d_r: 950
Episode: 0
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
Episode: 8
Episode: 9
minimum return: 1.0172429477102823e-07 maximum return: 258.2978321328874 average return: 26.17028203384699
==================================================
Evaluating d_r: 1000
Episode: 0
goal reached!
Episode: 1
Episode: 2
Episode: 3
Episode: 4
Episode: 5
Episode: 6
Episode: 7
goal reached!
Episode: 8
Episode: 9
minimum return: 1.3034418376331676e-06 maximum return: 481.1007558831062 average return: 115.2182847323918
Saved combined plot to condition5-ANTMAZE_BERT_MLP.png

============================================================
Final Success Rates per Model:
UDRLt_MLP0: 32.86%
UDRLt_MLP1: 28.10%
UDRLt_MLP2: 26.19%
UDRLt_MLP3: 25.24%
UDRLt_MLP4: 24.29%
UDRLt_MLP5: 21.43%

###############################################################################
Hbrk Cluster
Job 17922959 for user s5173019
Finished at: Sun Jun 15 00:02:27 CEST 2025

Job details:
============

Job ID                         : 17922959
Name                           : jobscript-gpu.sh
User                           : s5173019
Partition                      : gpumedium
Nodes                          : v100v2gpu1
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : COMPLETED  
Submit                         : 2025-06-14T13:57:34
Start                          : 2025-06-14T13:58:51
End                            : 2025-06-15T00:02:23
Reserved walltime              : 16:00:00
Used walltime                  : 10:03:32
Used CPU time                  : 09:57:26 (Efficiency: 12.37%)
% User (Computation)           : 99.86%
% System (I/O)                 :  0.14%
Total memory reserved          : 8000M
Maximum memory used            : 1.20G
Requested GPUs                 : 1
Allocated GPUs                 : v100=1
Max GPU utilization            : 30%
Max GPU memory used            : 582.00M

Acknowledgements:
=================

Please see this page for information about acknowledging Hbrk in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
